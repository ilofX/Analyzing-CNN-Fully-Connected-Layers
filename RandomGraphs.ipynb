{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Random Graph"
      ],
      "metadata": {
        "id": "KuRZ4wr4Di0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkit\n",
        "import networkit as nk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "IDJ1iIPiDhuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5o7kUQSgRpb"
      },
      "source": [
        "## Prepare Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps925AZLbSjy"
      },
      "outputs": [],
      "source": [
        "dimensions = np.load(\"./dimensions.npy\")\n",
        "print(\"Dimensions\",dimensions)\n",
        "input_layer_size = dimensions[0]\n",
        "first_layer_size = dimensions[1]\n",
        "second_layer_size = dimensions[3]\n",
        "third_layer_size = dimensions[5]\n",
        "\n",
        "distribution = np.load(\"./weights_distribution.npy\")\n",
        "print(\"Distribution\",distribution)\n",
        "mean1= distribution[0]\n",
        "var1= distribution[1]\n",
        "mean2 = distribution[2]\n",
        "var2 = distribution[3]\n",
        "mean3 = distribution[4]\n",
        "var3 = distribution[5]\n",
        "\n",
        "gradient_distribution = np.load(\"./gradient_distribution.npy\")\n",
        "print(\"Gradient Distribution\",gradient_distribution)\n",
        "mean1_grad= gradient_distribution[0]\n",
        "var1_grad= gradient_distribution[1]\n",
        "mean2_grad = gradient_distribution[2]\n",
        "var2_grad = gradient_distribution[3]\n",
        "mean3_grad = gradient_distribution[4]\n",
        "var3_grad = gradient_distribution[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creeate Graph"
      ],
      "metadata": {
        "id": "Ysa7FKJNFaut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = nk.Graph(input_layer_size + first_layer_size + second_layer_size + third_layer_size, weighted=True, directed=True)"
      ],
      "metadata": {
        "id": "0c6eh3jFFed9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Weights"
      ],
      "metadata": {
        "id": "RKSG-XjcFhKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add edges based on weights\n",
        "fc1_weights = np.ndarray(shape=(first_layer_size, input_layer_size))\n",
        "fc1_gradients = np.ndarray(shape=(first_layer_size, input_layer_size))\n",
        "\n",
        "# Input to First layer\n",
        "for i in tqdm(range(input_layer_size)):\n",
        "    for j in range(first_layer_size):\n",
        "      weight = np.random.normal(mean1,var1)\n",
        "      grad = np.random.poisson(mean1_grad)\n",
        "      fc1_weights[j][i]=weight\n",
        "      fc1_gradients[j][i]=grad\n",
        "      graph.addEdge(i, input_layer_size + j, weight)"
      ],
      "metadata": {
        "id": "QbN7cer3grZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fc2_weights = np.ndarray(shape=(second_layer_size,first_layer_size))\n",
        "fc2_gradients = np.ndarray(shape=(second_layer_size,first_layer_size))\n",
        "\n",
        "# First layer to second layer\n",
        "for i in tqdm(range(first_layer_size)):\n",
        "    for j in range(second_layer_size):\n",
        "        weight = np.random.normal(mean2, var2)\n",
        "        grad = np.random.poisson(mean2_grad)\n",
        "        fc2_weights[j][i]=weight\n",
        "        fc2_gradients[j][i]=grad\n",
        "        graph.addEdge(input_layer_size + i, input_layer_size + first_layer_size + j, weight)"
      ],
      "metadata": {
        "id": "5aaiZC_ouRhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fc3_weights = np.ndarray(shape=(third_layer_size, second_layer_size))\n",
        "fc3_gradients = np.ndarray(shape=(third_layer_size, second_layer_size))\n",
        "\n",
        "# Second layer to third layer\n",
        "for i in tqdm(range(second_layer_size)):\n",
        "    for j in range(third_layer_size):\n",
        "        weight = np.random.normal(mean3, var3)\n",
        "        grad = np.random.poisson(mean3_grad)\n",
        "        fc3_weights[j][i]=weight\n",
        "        fc3_gradients[j][i]=grad\n",
        "        graph.addEdge(input_layer_size + first_layer_size + i, input_layer_size + first_layer_size + second_layer_size + j, weight)"
      ],
      "metadata": {
        "id": "u_kQ5HXfuSgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check network is similar to real"
      ],
      "metadata": {
        "id": "AXSnj6cHFoAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you can use the graph object\n",
        "print(\"Nodes\",graph.numberOfNodes())\n",
        "print(\"Edges\",graph.numberOfEdges())"
      ],
      "metadata": {
        "id": "UgLZo9n5rJFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming fc1_weights is already defined as in your previous code\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(fc1_weights.flatten(), bins=3000, range=(-0.10, 0.10))\n",
        "plt.xlabel(\"Weight Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"FC1\")\n",
        "\n",
        "# Assuming fc2_weights is already defined as in your previous code\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(fc2_weights.flatten(), bins=3000, range=(-0.10, 0.10))\n",
        "plt.xlabel(\"Weight Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"FC2\")\n",
        "\n",
        "# Assuming fc3_weights is already defined as in your previous code\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(fc3_weights.flatten(), bins=3000, range=(-0.10, 0.10))\n",
        "plt.xlabel(\"Weight Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"FC3\")\n",
        "\n",
        "plt.subplots_adjust(left=0.1,\n",
        "                    bottom=0.1,\n",
        "                    right=0.9,\n",
        "                    top=0.9,\n",
        "                    wspace=0.9,\n",
        "                    hspace=0.9)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "\n",
        "plt.savefig(\"./output/random_weights_distribution.jpg\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sg3cII-URx6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming fc1_gradients_magnitude is already defined as in your previous code\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(fc1_gradients.flatten(), bins=300, range=(0, 0.50))\n",
        "plt.xlabel(\"Gradient Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"FC1\")\n",
        "\n",
        "# Assuming fc2_gradients_magnitude is already defined as in your previous code\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(fc2_gradients.flatten(), bins=200, range=(0, 0.50))\n",
        "plt.xlabel(\"Gradient Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"FC2\")\n",
        "\n",
        "# Assuming fc1_gradients_magnitude is already defined as in your previous code\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(fc3_gradients.flatten(), bins=200, range=(0, 0.50))\n",
        "plt.xlabel(\"Gradient Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"FC3\")\n",
        "\n",
        "plt.subplots_adjust(left=0.1,\n",
        "                    bottom=0.1,\n",
        "                    right=0.9,\n",
        "                    top=0.9,\n",
        "                    wspace=0.9,\n",
        "                    hspace=0.9)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "\n",
        "plt.savefig(\"./output/random_gradient_distribution.jpg\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "emOBNabZjX1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Embedding Features"
      ],
      "metadata": {
        "id": "SEAL1GyXEv7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimate Weighted Betweenness for nodes"
      ],
      "metadata": {
        "id": "gYczeYGa6Qcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute approximation of betweenness centrality\n",
        "betweenness = nk.centrality.EstimateBetweenness(graph, int(graph.numberOfNodes()*.35), normalized=True, parallel=True)\n",
        "\n",
        "betweenness.run()\n",
        "betweenness_scores = betweenness.scores()\n",
        "\n",
        "# Print or further process the betweenness centrality scores\n",
        "print(\"Estimated Betweenness Centrality Scores:\", betweenness_scores)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/random_estimated_betweenness.npy\", betweenness_scores)"
      ],
      "metadata": {
        "id": "8f1jZG76Sbwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Weighted Degree"
      ],
      "metadata": {
        "id": "M1xIiH09uOVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degrees = []\n",
        "\n",
        "for i in tqdm(range(input_layer_size + first_layer_size + second_layer_size + third_layer_size)):\n",
        "  degrees.append((abs(graph.weightedDegree(i))+abs(graph.weightedDegreeIn(i)))/(graph.degreeIn(i)+graph.degreeOut(i)))\n",
        "\n",
        "print(degrees)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/random_weighted_degree.npy\", degrees)"
      ],
      "metadata": {
        "id": "qLHJAd5dqyv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm = np.linalg.norm(degrees)\n",
        "if norm!=0:\n",
        "  normalized_degrees = degrees/norm\n",
        "\n",
        "print(normalized_degrees)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/random_normalized_degree.npy\", normalized_degrees)"
      ],
      "metadata": {
        "id": "EusOW-xwuHc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normalized Gradient"
      ],
      "metadata": {
        "id": "B2bqoN4nRv0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_node_gradients = []\n",
        "\n",
        "for i in tqdm(range(input_layer_size)):\n",
        "    weighted_node_gradients.append(np.sum(np.abs(np.multiply(fc1_gradients[:,i],fc1_weights[:,i]))))\n",
        "\n",
        "for i in tqdm(range(first_layer_size)):\n",
        "    weighted_node_gradients.append(np.sum(np.abs(np.multiply(fc2_gradients[:,i],fc2_weights[:,i]))))\n",
        "\n",
        "for i in tqdm(range(second_layer_size)):\n",
        "    weighted_node_gradients.append(np.sum(np.abs(np.multiply(fc3_gradients[:,i],fc3_weights[:,i]))))\n",
        "\n",
        "for i in tqdm(range(third_layer_size)):\n",
        "    weighted_node_gradients.append(0.0)\n",
        "\n",
        "print(weighted_node_gradients)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/random_weighted_gradient.npy\", weighted_node_gradients)"
      ],
      "metadata": {
        "id": "W87lMQYETDqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm = np.linalg.norm(weighted_node_gradients)\n",
        "if norm!=0:\n",
        "  normalized_weighted_node_gradients = weighted_node_gradients/norm\n",
        "\n",
        "print(normalized_weighted_node_gradients)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/random_normalized_gradient.npy\", normalized_weighted_node_gradients)"
      ],
      "metadata": {
        "id": "J3lMk6qyVlp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building graphs for paths computation**"
      ],
      "metadata": {
        "id": "ZotcSen96qHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Building network with absolute value of weights for shortest path computation"
      ],
      "metadata": {
        "id": "NXTYvP4Y6wVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abs_graph = nk.Graph(input_layer_size + first_layer_size + second_layer_size + third_layer_size, weighted=True, directed=True)\n",
        "# FC1\n",
        "for i in tqdm(range(input_layer_size)):\n",
        "    for j in range(first_layer_size):\n",
        "        abs_graph.addEdge(i, input_layer_size + j, abs(fc1_weights[j, i]))\n",
        "\n",
        "# FC2\n",
        "for i in tqdm(range(first_layer_size)):\n",
        "    for j in range(second_layer_size):\n",
        "        abs_graph.addEdge(input_layer_size + i, input_layer_size + first_layer_size + j, abs(fc2_weights[j,i]))\n",
        "\n",
        "# FC3\n",
        "for i in tqdm(range(second_layer_size)):\n",
        "    for j in range(third_layer_size):\n",
        "        abs_graph.addEdge(input_layer_size + first_layer_size + i, input_layer_size + first_layer_size+ second_layer_size + j, abs(fc3_weights[j,i]))"
      ],
      "metadata": {
        "id": "PJrvhMRu4Ibv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shortest Paths computation"
      ],
      "metadata": {
        "id": "e4I0AK4v65aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shortest_paths_distances = []   # Store distances separately\n",
        "shortest_paths_nodes = []       # Store paths (nodes) separately\n",
        "\n",
        "heuristic = [0 for _ in range(abs_graph.upperNodeIdBound())]\n",
        "\n",
        "for i in tqdm(range(0,input_layer_size)):\n",
        "    # print(\"searching for node\",i)\n",
        "    min_distance = float('inf')\n",
        "    min_path = []\n",
        "    dijkstra = nk.distance.Dijkstra(abs_graph, source=i, storePaths=True)\n",
        "    dijkstra.run()\n",
        "\n",
        "    for j in range(input_layer_size + first_layer_size + second_layer_size, input_layer_size + first_layer_size+ second_layer_size + third_layer_size):\n",
        "        # print(\"To node\",j)\n",
        "        if (dijkstra.distance(j)<min_distance):\n",
        "            min_distance = dijkstra.distance(j)\n",
        "            min_path = dijkstra.getPath(j)\n",
        "\n",
        "    # print(\"[\",min_distance,\",\",min_path,\"]\")\n",
        "    shortest_paths_distances.append(min_distance)\n",
        "    shortest_paths_nodes.append(min_path)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/random_shortest_paths_distances.npy\", shortest_paths_distances)\n",
        "np.save(\"./output/random_shortest_paths_nodes.npy\", shortest_paths_nodes)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yfpTUmEa68MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del(abs_graph)\n",
        "del(shortest_paths_distances)\n",
        "del(shortest_paths_nodes)\n",
        "del(dijkstra)"
      ],
      "metadata": {
        "id": "pjBiUBZX_dq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inverting weights to find longest paths"
      ],
      "metadata": {
        "id": "rnCcXtEk8DUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abs_graph = nk.Graph(input_layer_size + first_layer_size + second_layer_size + third_layer_size, weighted=True, directed=True)\n",
        "\n",
        "# FC1\n",
        "for i in tqdm(range(input_layer_size)):\n",
        "    for j in range(first_layer_size):\n",
        "        abs_graph.addEdge(i, input_layer_size + j, -abs(fc1_weights[j, i]))\n",
        "\n",
        "# FC2\n",
        "for i in tqdm(range(first_layer_size)):\n",
        "    for j in range(second_layer_size):\n",
        "        abs_graph.addEdge(input_layer_size + i, input_layer_size + first_layer_size + j, -abs(fc2_weights[j,i]))\n",
        "\n",
        "# FC3\n",
        "for i in tqdm(range(second_layer_size)):\n",
        "    for j in range(third_layer_size):\n",
        "        abs_graph.addEdge(input_layer_size + first_layer_size + i, input_layer_size + first_layer_size+ second_layer_size + j, -abs(fc3_weights[j,i]))"
      ],
      "metadata": {
        "id": "0MEsBVBm8Cxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding longest_paths"
      ],
      "metadata": {
        "id": "L7WFNUBj8NjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "longest_paths_distances = []   # Store distances separately\n",
        "longest_paths_nodes = []       # Store paths (nodes) separately\n",
        "\n",
        "heuristic = [0 for _ in range(abs_graph.upperNodeIdBound())]\n",
        "\n",
        "for i in tqdm(range(0,input_layer_size)):\n",
        "    # print(\"searching for node\",i)\n",
        "    min_distance = float('-inf')\n",
        "    min_path = []\n",
        "    dijkstra = nk.distance.Dijkstra(abs_graph, source=i, storePaths=True)\n",
        "    dijkstra.run()\n",
        "\n",
        "    for j in range(input_layer_size + first_layer_size+ second_layer_size, input_layer_size + first_layer_size+ second_layer_size + third_layer_size):\n",
        "        # print(\"To node\",j)\n",
        "        if (dijkstra.distance(j)>min_distance):\n",
        "            min_distance = dijkstra.distance(j)\n",
        "            min_path = dijkstra.getPath(j)\n",
        "\n",
        "    # print(\"[\",-min_distance,\",\",min_path,\"]\")\n",
        "    longest_paths_distances.append(-min_distance)\n",
        "    longest_paths_nodes.append(min_path)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/random_longest_paths_distances.npy\", longest_paths_distances)\n",
        "np.save(\"./output/random_longest_paths_nodes.npy\", longest_paths_nodes)"
      ],
      "metadata": {
        "id": "ZyfYEMzi8P6j",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compress Output Folder"
      ],
      "metadata": {
        "id": "ARNj4dDrjgId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/Random_Results.zip /content/output"
      ],
      "metadata": {
        "id": "M5JoD8O-jilk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
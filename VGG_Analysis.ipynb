{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependencies"
      ],
      "metadata": {
        "id": "2WdLBHCbRfxd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI3lq8SLaaLL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading pre trained model of VGG16"
      ],
      "metadata": {
        "id": "zz6XDS5hRi_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VGG = models.vgg16(pretrained=True)"
      ],
      "metadata": {
        "id": "sAZIW_G1aiuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract information about fully connected layers"
      ],
      "metadata": {
        "id": "3XEithObRpC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the fully connected layers\n",
        "fc1 = VGG.classifier[0]\n",
        "fc2 = VGG.classifier[3]\n",
        "fc3 = VGG.classifier[6]\n",
        "\n",
        "# Extract the weight matrices\n",
        "fc1_weights = fc1.weight.detach().numpy()\n",
        "fc2_weights = fc2.weight.detach().numpy()\n",
        "fc3_weights = fc3.weight.detach().numpy()\n",
        "\n",
        "print(fc1_weights.shape)\n",
        "print(\"Mean1 \"+ str(np.mean(fc1_weights)))\n",
        "print(\"Var1 \" + str(np.var(fc1_weights)))\n",
        "print(\"Max1 \" + str(np.max(fc1_weights)) + \" Min1 \" + str(np.min(fc1_weights)))\n",
        "print(\"\")\n",
        "\n",
        "print(fc2_weights.shape)\n",
        "print(\"Mean2 \"+ str(np.mean(fc2_weights)))\n",
        "print(\"Var2 \" + str(np.var(fc2_weights)))\n",
        "print(\"Max2 \" + str(np.max(fc2_weights)) + \" Min2 \" + str(np.min(np.abs(fc2_weights))))\n",
        "print(str(np.count_nonzero(fc2_weights[ np.where( fc2_weights >= np.mean(fc2_weights)-0.002 ) ]))+\" out of \" +str(4096*4096))\n",
        "print(\"\")\n",
        "\n",
        "print(fc3_weights.shape)\n",
        "print(\"Mean3 \"+ str(np.mean(fc3_weights)))\n",
        "print(\"Var3 \" + str(np.var(fc3_weights)))\n",
        "print(\"Max3 \" + str(np.max(fc3_weights)) + \" Min3 \" + str(np.min(fc3_weights)))"
      ],
      "metadata": {
        "id": "mzEil_n0ajWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dimensions = [fc1_weights.shape[1],fc1_weights.shape[0],fc2_weights.shape[1],fc2_weights.shape[0],fc3_weights.shape[1],fc3_weights.shape[0]]\n",
        "print(dimensions)\n",
        "weights_distributions = [np.mean(fc1_weights),np.std(fc1_weights),np.mean(fc2_weights),np.std(fc2_weights),np.mean(fc3_weights),np.std(fc3_weights)]\n",
        "print(weights_distributions)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "\n",
        "np.save(\"./output/vgg_dimensions.npy\", dimensions)\n",
        "np.save(\"./output/vgg_weights_distribution.npy\", weights_distributions)"
      ],
      "metadata": {
        "id": "vlwHQS5xCNeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot histograms of discretized weights values"
      ],
      "metadata": {
        "id": "mdx8J0A3Rvr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming fc1_weights is already defined as in your previous code\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(fc1_weights.flatten(), bins=3000, range=(-0.10, 0.10))\n",
        "plt.xlabel(\"Weight Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"FC1\")\n",
        "\n",
        "# Assuming fc2_weights is already defined as in your previous code\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(fc2_weights.flatten(), bins=3000, range=(-0.10, 0.10))\n",
        "plt.xlabel(\"Weight Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"FC2\")\n",
        "\n",
        "# Assuming fc3_weights is already defined as in your previous code\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(fc3_weights.flatten(), bins=3000, range=(-0.10, 0.10))\n",
        "plt.xlabel(\"Weight Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"FC3\")\n",
        "\n",
        "plt.subplots_adjust(left=0.1,\n",
        "                    bottom=0.1,\n",
        "                    right=0.9,\n",
        "                    top=0.9,\n",
        "                    wspace=0.9,\n",
        "                    hspace=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Vm-s4sWfV_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the CINIC-10 dataset"
      ],
      "metadata": {
        "id": "hhZ6umUn2hVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz\n",
        "!tar -xzvf CINIC-10.tar.gz"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qm9TmC2bMI1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the transformations for the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),  # Resize the image to 256x256\n",
        "    transforms.CenterCrop(224),  # Crop the image to 224x224\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize the image\n",
        "])\n",
        "\n",
        "\n",
        "# Load the CINIC-10 dataset\n",
        "train_dataset = datasets.ImageFolder(root='./train', transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root='./test', transform=transform)\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Now you can use train_loader and test_loader to iterate through the dataset\n",
        "# Example:\n",
        "for images, labels in train_loader:\n",
        "    print(\"Image batch shape:\", images.shape)\n",
        "    print(\"Label batch shape:\", labels.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "ZyvijkbjKcKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate with the CINIC-10 dataset ang compute average gradient for the edges"
      ],
      "metadata": {
        "id": "i8-3U3TYLx-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store gradients\n",
        "fc1_avg_gradients = []\n",
        "fc2_avg_gradients = []\n",
        "fc3_avg_gradients = []\n",
        "#gradients = []\n",
        "\n",
        "# Set the device to cuda if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set the device globally\n",
        "torch.set_default_device(device)\n",
        "\n",
        "# Iterate through the first 30 images in the test dataset containing 1407 images\n",
        "\n",
        "for i, (images, labels) in enumerate(tqdm(test_loader)):\n",
        "    if i%70 != 0:\n",
        "      continue\n",
        "\n",
        "    images.requires_grad_(True) # Enable gradient calculation for the image input\n",
        "\n",
        "    # Perform the forward pass\n",
        "    outputs = VGG(images)\n",
        "\n",
        "    # Choose a class to calculate the gradient with respect to\n",
        "    target_class = labels[0]  # Using the actual label\n",
        "\n",
        "    # Calculate the gradient of the output with respect to the image\n",
        "    loss = outputs[0, target_class]  # loss is the output score for the target class\n",
        "    loss.backward()\n",
        "\n",
        "    fc1_avg_gradients.append(fc1.weight.grad.data.numpy())\n",
        "    fc2_avg_gradients.append(fc2.weight.grad.data.numpy())\n",
        "    fc3_avg_gradients.append(fc3.weight.grad.data.numpy())\n",
        "    # print(\"Processed image\",(i/70)+1,\"/30\",\"Target class:\",target_class)\n",
        "    #gradient = images.grad.data.numpy()\n",
        "    #gradients.append(gradient)\n",
        "\n",
        "#average_gradient = np.mean(np.array(gradients), axis = 0)\n",
        "fc1_avg_gradients = np.mean(np.array(fc1_avg_gradients), axis=0)\n",
        "fc2_avg_gradients = np.mean(np.array(fc2_avg_gradients), axis=0)\n",
        "fc3_avg_gradients = np.mean(np.array(fc3_avg_gradients), axis=0)\n",
        "\n",
        "# Example of calculating magnitudes (absolute values) of the gradient\n",
        "#gardient_magnitude = np.abs(average_gradient)\n",
        "fc1_gradients_magnitude = np.abs(fc1_avg_gradients)\n",
        "fc2_gradients_magnitude = np.abs(fc2_avg_gradients)\n",
        "fc3_gradients_magnitude = np.abs(fc3_avg_gradients)"
      ],
      "metadata": {
        "id": "uG1XRpqjNc-g",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fc1_gradients_magnitude.shape)\n",
        "print(\"Mean1 \"+ str(np.mean(fc1_gradients_magnitude)))\n",
        "print(\"Var1 \" + str(np.var(fc1_gradients_magnitude)))\n",
        "print(\"Max1 \" + str(np.max(fc1_gradients_magnitude)) + \" Min1 \" + str(np.min(fc1_gradients_magnitude)))\n",
        "print(\"\")\n",
        "\n",
        "print(fc2_gradients_magnitude.shape)\n",
        "print(\"Mean2 \"+ str(np.mean(fc2_gradients_magnitude)))\n",
        "print(\"Var2 \" + str(np.var(fc2_gradients_magnitude)))\n",
        "print(\"Max2 \" + str(np.max(fc2_gradients_magnitude)) + \" Min2 \" + str(np.min(np.abs(fc2_gradients_magnitude))))\n",
        "print(\"\")\n",
        "\n",
        "print(fc3_gradients_magnitude.shape)\n",
        "print(\"Mean3 \"+ str(np.mean(fc3_gradients_magnitude)))\n",
        "print(\"Var3 \" + str(np.var(fc3_gradients_magnitude)))\n",
        "print(\"Max3 \" + str(np.max(fc3_gradients_magnitude)) + \" Min3 \" + str(np.min(fc3_gradients_magnitude)))"
      ],
      "metadata": {
        "id": "7RlPvN49gIHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming fc1_weights is already defined as in your previous code\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(fc1_gradients_magnitude.flatten(), bins=100, range=(-0.10, 0.10))\n",
        "plt.xlabel(\"Gradient Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"FC1\")\n",
        "\n",
        "# Assuming fc2_weights is already defined as in your previous code\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(fc2_gradients_magnitude.flatten(), bins=100, range=(-0.10, 0.10))\n",
        "plt.xlabel(\"Gradient Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"FC2\")\n",
        "\n",
        "# Assuming fc3_weights is already defined as in your previous code\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(fc3_gradients_magnitude.flatten(), bins=100, range=(-0.10, 0.10))\n",
        "plt.xlabel(\"Gradient Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"FC3\")\n",
        "\n",
        "plt.subplots_adjust(left=0.1,\n",
        "                    bottom=0.1,\n",
        "                    right=0.9,\n",
        "                    top=0.9,\n",
        "                    wspace=0.9,\n",
        "                    hspace=0.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "frAh3HinWe3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_distributions = [np.mean(fc1_gradients_magnitude),np.std(fc1_gradients_magnitude),np.mean(fc2_gradients_magnitude),np.std(fc2_gradients_magnitude),np.mean(fc3_gradients_magnitude),np.std(fc3_gradients_magnitude)]\n",
        "print(gradient_distributions)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "\n",
        "np.save(\"./output/vgg_gradient_distribution.npy\", gradient_distributions)"
      ],
      "metadata": {
        "id": "3hgfuVA4CW27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph for the three FC layers of VGG\n",
        "### Build the nodes"
      ],
      "metadata": {
        "id": "EEdMFXAQAmfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkit\n",
        "import networkit as nk"
      ],
      "metadata": {
        "id": "mqupr6xbmEG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of neurons in each fully connected layer\n",
        "num_neurons_input = fc1.in_features\n",
        "num_neurons_fc1 = fc1.out_features\n",
        "num_neurons_fc2 = fc2.out_features\n",
        "num_neurons_fc3 = fc3.out_features\n",
        "\n",
        "# Create an empty graph\n",
        "graph = nk.Graph(num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + num_neurons_fc3, weighted=True, directed=True)"
      ],
      "metadata": {
        "id": "scQlRgKGBAK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Draw the edges"
      ],
      "metadata": {
        "id": "sBnmLPcYBBAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(num_neurons_input)):\n",
        "    for j in range(num_neurons_fc1):\n",
        "        graph.addEdge(i, num_neurons_input + j, fc1_weights[j, i])"
      ],
      "metadata": {
        "id": "vBlzGjuyrEOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(num_neurons_fc1)):\n",
        "    for j in range(num_neurons_fc2):\n",
        "        graph.addEdge(num_neurons_input + i, num_neurons_input + num_neurons_fc1 + j, fc2_weights[j,i])"
      ],
      "metadata": {
        "id": "RCimqeWrrD6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(num_neurons_fc2)):\n",
        "    for j in range(num_neurons_fc3):\n",
        "        graph.addEdge(num_neurons_input + num_neurons_fc1 + i, num_neurons_input + num_neurons_fc1+ num_neurons_fc2 + j, fc3_weights[j,i])"
      ],
      "metadata": {
        "id": "5oejlsgGqUMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding Features"
      ],
      "metadata": {
        "id": "hlwnIjEMNf4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimate Weighted Betweenness for nodes"
      ],
      "metadata": {
        "id": "Ay18XmAENlRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(int(graph.numberOfNodes()*.35))"
      ],
      "metadata": {
        "id": "Iv-iKSmvBYjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute approximation of betweenness centrality\n",
        "betweenness = nk.centrality.EstimateBetweenness(graph, int(graph.numberOfNodes()*.35), normalized=True, parallel=True)\n",
        "\n",
        "betweenness.run()\n",
        "betweenness_scores = betweenness.scores()\n",
        "\n",
        "# Print or further process the betweenness centrality scores\n",
        "print(\"Estimated Betweenness Centrality Scores:\", betweenness_scores)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/vgg_betweennes.npy\", betweenness_scores)"
      ],
      "metadata": {
        "id": "SI7_15xIP43v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute weighted degree of nodes"
      ],
      "metadata": {
        "id": "TvmF0gt6hw-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degrees = []\n",
        "\n",
        "for i in tqdm(range(num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + num_neurons_fc3)):\n",
        "  degrees.append((abs(graph.weightedDegree(i))+abs(graph.weightedDegreeIn(i)))/(graph.degreeIn(i)+graph.degreeOut(i)))\n",
        "\n",
        "print(degrees)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/vgg_weighted_degree.npy\", degrees)"
      ],
      "metadata": {
        "id": "bysgnlcTIlYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm = np.linalg.norm(degrees)\n",
        "if norm!=0:\n",
        "  normalized_degrees = degrees/norm\n",
        "\n",
        "print(normalized_degrees)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/vgg_normalized_degree.npy\", normalized_degrees)"
      ],
      "metadata": {
        "id": "MPem8h2wSlWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute weighted gradient for nodes"
      ],
      "metadata": {
        "id": "Zm_PEOPWIl0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_node_gradients = []\n",
        "\n",
        "for i in tqdm(range(num_neurons_input)):\n",
        "    weighted_node_gradients.append(np.sum(np.abs(np.multiply(fc1_gradients_magnitude[:,i],fc1_weights[:,i]))))\n",
        "\n",
        "for i in tqdm(range(num_neurons_fc1)):\n",
        "    weighted_node_gradients.append(np.sum(np.abs(np.multiply(fc2_gradients_magnitude[:,i],fc2_weights[:,i]))))\n",
        "\n",
        "for i in tqdm(range(num_neurons_fc2)):\n",
        "    weighted_node_gradients.append(np.sum(np.abs(np.multiply(fc3_gradients_magnitude[:,i],fc3_weights[:,i]))))\n",
        "\n",
        "for i in tqdm(range(num_neurons_fc3)):\n",
        "    weighted_node_gradients.append(0.0)\n",
        "\n",
        "print(weighted_node_gradients)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/vgg_weighted_gradient.npy\", weighted_node_gradients)"
      ],
      "metadata": {
        "id": "tGXjm1eeNPxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm = np.linalg.norm(weighted_node_gradients)\n",
        "if norm!=0:\n",
        "  normalized_weighted_node_gradients = weighted_node_gradients/norm\n",
        "\n",
        "print(normalized_weighted_node_gradients)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/vgg_normalized_gradient.npy\", normalized_weighted_node_gradients)"
      ],
      "metadata": {
        "id": "B_Fycnffdr0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add embedding features to the graph"
      ],
      "metadata": {
        "id": "biKm-fpTeUh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "node_betweenness = graph.attachNodeAttribute(\"betweenness\", float)\n",
        "node_degree = graph.attachNodeAttribute(\"degree\", float)\n",
        "node_gradient = graph.attachNodeAttribute(\"gradient\", float)"
      ],
      "metadata": {
        "id": "6DV0zmvNeXny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + num_neurons_fc3):\n",
        "    node_betweenness[i]=betweenness_scores[i]\n",
        "    node_degree[i]=normalized_degrees[i]\n",
        "    node_gradient[i]=normalized_weighted_node_gradients[i]"
      ],
      "metadata": {
        "id": "az_Ck7yyfdkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save complete network to file"
      ],
      "metadata": {
        "id": "jffMPEEZ3byn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "\n",
        "nk.writeGraph(graph,\"./output/vgg_FC_directed.nkbg003\", nk.Format.NetworkitBinary, chunks=16, NetworkitBinaryWeights=2)"
      ],
      "metadata": {
        "id": "WKGdrLq42dx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del(graph)"
      ],
      "metadata": {
        "id": "ELb2eQF45EFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building graphs for paths computation**"
      ],
      "metadata": {
        "id": "ZotcSen96qHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Building network with absolute value of weights for shortest path computation"
      ],
      "metadata": {
        "id": "NXTYvP4Y6wVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abs_graph = nk.Graph(num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + num_neurons_fc3, weighted=True, directed=True)\n",
        "\n",
        "# FC1\n",
        "for i in tqdm(range(num_neurons_input)):\n",
        "    for j in range(num_neurons_fc1):\n",
        "        abs_graph.addEdge(i, num_neurons_input + j, abs(fc1_weights[j, i]))\n",
        "\n",
        "# FC2\n",
        "for i in tqdm(range(num_neurons_fc1)):\n",
        "    for j in range(num_neurons_fc2):\n",
        "        abs_graph.addEdge(num_neurons_input + i, num_neurons_input + num_neurons_fc1 + j, abs(fc2_weights[j,i]))\n",
        "\n",
        "# FC3\n",
        "for i in tqdm(range(num_neurons_fc2)):\n",
        "    for j in range(num_neurons_fc3):\n",
        "        abs_graph.addEdge(num_neurons_input + num_neurons_fc1 + i, num_neurons_input + num_neurons_fc1+ num_neurons_fc2 + j, abs(fc3_weights[j,i]))"
      ],
      "metadata": {
        "id": "PJrvhMRu4Ibv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shortest Paths computation"
      ],
      "metadata": {
        "id": "e4I0AK4v65aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shortest_paths_distances = []   # Store distances separately\n",
        "shortest_paths_nodes = []       # Store paths (nodes) separately\n",
        "\n",
        "heuristic = [0 for _ in range(abs_graph.upperNodeIdBound())]\n",
        "\n",
        "for i in tqdm(range(0,num_neurons_input)):\n",
        "    # print(\"searching for node\",i)\n",
        "    min_distance = float('inf')\n",
        "    min_path = []\n",
        "    dijkstra = nk.distance.Dijkstra(abs_graph, source=i, storePaths=True)\n",
        "    dijkstra.run()\n",
        "\n",
        "    for j in range(num_neurons_input + num_neurons_fc1 + num_neurons_fc2, num_neurons_input + num_neurons_fc1+ num_neurons_fc2 + num_neurons_fc3):\n",
        "        # print(\"To node\",j)\n",
        "        if (dijkstra.distance(j)<min_distance):\n",
        "            min_distance = dijkstra.distance(j)\n",
        "            min_path = dijkstra.getPath(j)\n",
        "\n",
        "    #print(\"[\",min_distance,\",\",min_path,\"]\")\n",
        "    shortest_paths_distances.append(min_distance)\n",
        "    shortest_paths_nodes.append(min_path)\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/vgg_shortest_paths_distances.npy\", shortest_paths_distances)\n",
        "np.save(\"./output/vgg_shortest_paths_nodes.npy\", shortest_paths_nodes)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yfpTUmEa68MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del(abs_graph)\n",
        "del(shortest_paths_distances)\n",
        "del(shortest_paths_nodes)\n",
        "del(dijkstra)"
      ],
      "metadata": {
        "id": "gXvNTGEk5CKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inverting weights to find longest paths"
      ],
      "metadata": {
        "id": "rnCcXtEk8DUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abs_graph = nk.Graph(num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + num_neurons_fc3, weighted=True, directed=True)\n",
        "\n",
        "# FC1\n",
        "for i in tqdm(range(num_neurons_input)):\n",
        "    for j in range(num_neurons_fc1):\n",
        "        abs_graph.addEdge(i, num_neurons_input + j, -abs(fc1_weights[j, i]))\n",
        "\n",
        "# FC2\n",
        "for i in tqdm(range(num_neurons_fc1)):\n",
        "    for j in range(num_neurons_fc2):\n",
        "        abs_graph.addEdge(num_neurons_input + i, num_neurons_input + num_neurons_fc1 + j, -abs(fc2_weights[j,i]))\n",
        "\n",
        "# FC3\n",
        "for i in tqdm(range(num_neurons_fc2)):\n",
        "    for j in range(num_neurons_fc3):\n",
        "        abs_graph.addEdge(num_neurons_input + num_neurons_fc1 + i, num_neurons_input + num_neurons_fc1+ num_neurons_fc2 + j, -abs(fc3_weights[j,i]))"
      ],
      "metadata": {
        "id": "0MEsBVBm8Cxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding longest_paths"
      ],
      "metadata": {
        "id": "wLuqxh24iywy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "longest_paths_distances = []   # Store distances separately\n",
        "longest_paths_nodes = []       # Store paths (nodes) separately\n",
        "\n",
        "heuristic = [0 for _ in range(abs_graph.upperNodeIdBound())]\n",
        "\n",
        "for i in tqdm(range(0,num_neurons_input)):\n",
        "    # print(\"searching for node\",i)\n",
        "    min_distance = float('-inf')\n",
        "    min_path = []\n",
        "    dijkstra = nk.distance.Dijkstra(abs_graph, source=i, storePaths=True)\n",
        "    dijkstra.run()\n",
        "\n",
        "    for j in range(num_neurons_input + num_neurons_fc1+ num_neurons_fc2, num_neurons_input + num_neurons_fc1+ num_neurons_fc2 + num_neurons_fc3):\n",
        "        # print(\"To node\",j)\n",
        "        if (dijkstra.distance(j)>min_distance):\n",
        "            min_distance = dijkstra.distance(j)\n",
        "            min_path = dijkstra.getPath(j)\n",
        "\n",
        "    #print(\"[\",-min_distance,\",\",min_path,\"]\")\n",
        "    longest_paths_distances.append(-min_distance)\n",
        "    longest_paths_nodes.append(min_path)\n",
        "\n",
        "\n",
        "if not os.path.isdir('./output/'):\n",
        "    os.makedirs('./output')\n",
        "np.save(\"./output/vgg_longest_paths_distances.npy\", longest_paths_distances)\n",
        "np.save(\"./output/vgg_longest_paths_nodes.npy\", longest_paths_nodes)"
      ],
      "metadata": {
        "id": "ZyfYEMzi8P6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del(abs_graph)\n",
        "del(longest_paths_distances)\n",
        "del(longest_paths_nodes)\n",
        "del(dijkstra)"
      ],
      "metadata": {
        "id": "bUPtyeVJ4-iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compress Output Folder"
      ],
      "metadata": {
        "id": "LLYBnbBNc0Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/VGG_Results.zip /content/output"
      ],
      "metadata": {
        "id": "69qFKC5Zcgq4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
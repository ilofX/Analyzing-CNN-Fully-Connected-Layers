{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2rzdLXwYEAQ"
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEne5r73VNUc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iziTxw_dYJdh"
   },
   "source": [
    "# Loading pre trained model of AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-BP0unAYQIp"
   },
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsb1nmRnYn4B"
   },
   "source": [
    "# Extract information about fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi72K1CsY1Wk"
   },
   "outputs": [],
   "source": [
    "# Access the fully connected layers\n",
    "fc1 = alexnet.classifier[1]\n",
    "fc2 = alexnet.classifier[4]\n",
    "fc3 = alexnet.classifier[6]\n",
    "\n",
    "# Extract the weight matrices\n",
    "fc1_weights = fc1.weight.detach().numpy()\n",
    "fc2_weights = fc2.weight.detach().numpy()\n",
    "fc3_weights = fc3.weight.detach().numpy()\n",
    "\n",
    "cutting_factor = 0\n",
    "\n",
    "print(fc1_weights.shape)\n",
    "print(\"Mean1 \" + str(np.mean(fc1_weights)))\n",
    "print(\"Var1 \" + str(np.var(fc1_weights)))\n",
    "print(\"Max1 \" + str(np.max(fc1_weights)) + \" Min1 \" + str(np.min(fc1_weights)))\n",
    "print(\"\")\n",
    "\n",
    "print(fc2_weights.shape)\n",
    "print(\"Mean2 \" + str(np.mean(fc2_weights)))\n",
    "print(\"Var2 \" + str(np.var(fc2_weights)))\n",
    "print(\"Max2 \" + str(np.max(fc2_weights)) + \" Min2 \" + str(np.min(np.abs(fc2_weights))))\n",
    "print(\"\")\n",
    "\n",
    "print(fc3_weights.shape)\n",
    "print(\"Mean3 \" + str(np.mean(fc3_weights)))\n",
    "print(\"Var3 \" + str(np.var(fc3_weights)))\n",
    "print(\"Max3 \" + str(np.max(fc3_weights)) + \" Min3 \" + str(np.min(fc3_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iAkmuc0AeS2"
   },
   "outputs": [],
   "source": [
    "dimensions = [fc1_weights.shape[1], fc1_weights.shape[0], fc2_weights.shape[1], fc2_weights.shape[0],\n",
    "              fc3_weights.shape[1], fc3_weights.shape[0]]\n",
    "print(dimensions)\n",
    "weights_distributions = [np.mean(fc1_weights), np.std(fc1_weights), np.mean(fc2_weights), np.std(fc2_weights),\n",
    "                         np.mean(fc3_weights), np.std(fc3_weights)]\n",
    "print(weights_distributions)\n",
    "\n",
    "if not os.path.isdir('./output/'):\n",
    "    os.makedirs('./output')\n",
    "\n",
    "np.save(\"./output/alexnet_dimensions.npy\", dimensions)\n",
    "np.save(\"./output/alexnet_weights_distribution.npy\", weights_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am6sVb3vRr2i"
   },
   "source": [
    "# Plot histograms of discretized weights values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sg3cII-URx6t"
   },
   "outputs": [],
   "source": [
    "# Assuming fc1_weights is already defined as in your previous code\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(fc1_weights.flatten(), bins=3000, range=(-0.10, 0.10))\n",
    "plt.xlabel(\"Weight Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"FC1\")\n",
    "\n",
    "# Assuming fc2_weights is already defined as in your previous code\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(fc2_weights.flatten(), bins=3000, range=(-0.10, 0.10))\n",
    "plt.xlabel(\"Weight Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"FC2\")\n",
    "\n",
    "# Assuming fc3_weights is already defined as in your previous code\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(fc3_weights.flatten(), bins=3000, range=(-0.10, 0.10))\n",
    "plt.xlabel(\"Weight Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"FC3\")\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1,\n",
    "                    right=0.9,\n",
    "                    top=0.9,\n",
    "                    wspace=0.9,\n",
    "                    hspace=0.9)\n",
    "\n",
    "if not os.path.isdir('./output/'):\n",
    "    os.makedirs('./output')\n",
    "\n",
    "plt.savefig(\"./output/alexnet_weights_distribution.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhZ6umUn2hVF"
   },
   "source": [
    "# Load the CINIC-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "qm9TmC2bMI1F"
   },
   "outputs": [],
   "source": [
    "!wget https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz\n",
    "!tar -xzvf CINIC-10.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyvijkbjKcKV"
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transformations for the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image to 256x256\n",
    "    transforms.CenterCrop(224),  # Crop the image to 224x224\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Load the CINIC-10 dataset\n",
    "train_dataset = datasets.ImageFolder(root='./train', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='./test', transform=transform)\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Now you can use train_loader and test_loader to iterate through the dataset\n",
    "# Example:\n",
    "for images, labels in train_loader:\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8-3U3TYLx-f"
   },
   "source": [
    "# Evaluate with the CINIC-10 dataset ang compute average gradient for the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "uG1XRpqjNc-g"
   },
   "outputs": [],
   "source": [
    "# Initialize a list to store gradients\n",
    "fc1_avg_gradients = []\n",
    "fc2_avg_gradients = []\n",
    "fc3_avg_gradients = []\n",
    "#gradients = []\n",
    "\n",
    "# Set the device to cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the device globally\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Iterate through the first 30 images in the test dataset containing 1407 images\n",
    "\n",
    "for i, (images, labels) in enumerate(tqdm(test_loader)):\n",
    "    if i % 70 != 0:\n",
    "        continue\n",
    "\n",
    "    images.requires_grad_(True)  # Enable gradient calculation for the image input\n",
    "\n",
    "    # Perform the forward pass\n",
    "    outputs = alexnet(images)\n",
    "\n",
    "    # Choose a class to calculate the gradient with respect to\n",
    "    target_class = labels[0]  # Using the actual label\n",
    "\n",
    "    # Calculate the gradient of the output with respect to the image\n",
    "    loss = outputs[0, target_class]  # loss is the output score for the target class\n",
    "    loss.backward()\n",
    "\n",
    "    fc1_avg_gradients.append(fc1.weight.grad.data.numpy())\n",
    "    fc2_avg_gradients.append(fc2.weight.grad.data.numpy())\n",
    "    fc3_avg_gradients.append(fc3.weight.grad.data.numpy())\n",
    "    # print(\"Processed image\",(i/70)+1,\"/30\",\"Target class:\",target_class)\n",
    "    #gradient = images.grad.data.numpy()\n",
    "    #gradients.append(gradient)\n",
    "\n",
    "#average_gradient = np.mean(np.array(gradients), axis = 0)\n",
    "fc1_avg_gradients = np.mean(np.array(fc1_avg_gradients), axis=0)\n",
    "fc2_avg_gradients = np.mean(np.array(fc2_avg_gradients), axis=0)\n",
    "fc3_avg_gradients = np.mean(np.array(fc3_avg_gradients), axis=0)\n",
    "\n",
    "# Example of calculating magnitudes (absolute values) of the gradient\n",
    "#gardient_magnitude = np.abs(average_gradient)\n",
    "fc1_gradients_magnitude = np.abs(fc1_avg_gradients)\n",
    "fc2_gradients_magnitude = np.abs(fc2_avg_gradients)\n",
    "fc3_gradients_magnitude = np.abs(fc3_avg_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RlPvN49gIHx"
   },
   "outputs": [],
   "source": [
    "print(fc1_gradients_magnitude.shape)\n",
    "print(\"Mean1 \" + str(np.mean(fc1_gradients_magnitude)))\n",
    "print(\"Var1 \" + str(np.var(fc1_gradients_magnitude)))\n",
    "print(\"Max1 \" + str(np.max(fc1_gradients_magnitude)) + \" Min1 \" + str(np.min(fc1_gradients_magnitude)))\n",
    "print(\"\")\n",
    "\n",
    "print(fc2_gradients_magnitude.shape)\n",
    "print(\"Mean2 \" + str(np.mean(fc2_gradients_magnitude)))\n",
    "print(\"Var2 \" + str(np.var(fc2_gradients_magnitude)))\n",
    "print(\"Max2 \" + str(np.max(fc2_gradients_magnitude)) + \" Min2 \" + str(np.min(np.abs(fc2_gradients_magnitude))))\n",
    "print(\"\")\n",
    "\n",
    "print(fc3_gradients_magnitude.shape)\n",
    "print(\"Mean3 \" + str(np.mean(fc3_gradients_magnitude)))\n",
    "print(\"Var3 \" + str(np.var(fc3_gradients_magnitude)))\n",
    "print(\"Max3 \" + str(np.max(fc3_gradients_magnitude)) + \" Min3 \" + str(np.min(fc3_gradients_magnitude)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUCw3fHiMC-W"
   },
   "outputs": [],
   "source": [
    "# Assuming fc1_gradients_magnitude is already defined as in your previous code\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(fc1_gradients_magnitude.flatten(), bins=300, range=(0, 0.50))\n",
    "plt.xlabel(\"Gradient Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"FC1\")\n",
    "\n",
    "# Assuming fc2_gradients_magnitude is already defined as in your previous code\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(fc2_gradients_magnitude.flatten(), bins=200, range=(0, 0.50))\n",
    "plt.xlabel(\"Gradient Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"FC2\")\n",
    "\n",
    "# Assuming fc1_gradients_magnitude is already defined as in your previous code\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(fc3_gradients_magnitude.flatten(), bins=200, range=(0, 0.50))\n",
    "plt.xlabel(\"Gradient Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"FC3\")\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1,\n",
    "                    right=0.9,\n",
    "                    top=0.9,\n",
    "                    wspace=0.9,\n",
    "                    hspace=0.9)\n",
    "\n",
    "if not os.path.isdir('./output/'):\n",
    "    os.makedirs('./output')\n",
    "\n",
    "plt.savefig(\"./output/alexnet_gradient_distribution.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGf9dnhdBwFm"
   },
   "outputs": [],
   "source": [
    "gradient_distributions = [np.mean(fc1_gradients_magnitude), np.std(fc1_gradients_magnitude),\n",
    "                          np.mean(fc2_gradients_magnitude), np.std(fc2_gradients_magnitude),\n",
    "                          np.mean(fc3_gradients_magnitude), np.std(fc3_gradients_magnitude)]\n",
    "print(gradient_distributions)\n",
    "\n",
    "if not os.path.isdir('./output/'):\n",
    "    os.makedirs('./output')\n",
    "\n",
    "np.save(\"./output/alexnet_gradient_distribution.npy\", gradient_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEdMFXAQAmfE"
   },
   "source": [
    "# Building the graph for the last three fully connected layers of pretrained alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAtZ50TD6f6J"
   },
   "source": [
    "## Build the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "qvDjggJipQiE"
   },
   "outputs": [],
   "source": [
    "!pip install networkit\n",
    "import networkit as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scQlRgKGBAK5"
   },
   "outputs": [],
   "source": [
    "# Assuming fc1, fc2, and fc3 are defined as in your previous code\n",
    "# Get the number of neurons in each fully connected layer\n",
    "num_neurons_input = fc1.in_features\n",
    "num_neurons_fc1 = fc1.out_features\n",
    "num_neurons_fc2 = fc2.out_features\n",
    "num_neurons_fc3 = fc3.out_features\n",
    "\n",
    "# Create an empty graph\n",
    "graph = nk.Graph(num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + num_neurons_fc3, weighted=True, directed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBnmLPcYBBAp"
   },
   "source": [
    "## Draw the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBlzGjuyrEOL"
   },
   "outputs": [],
   "source": [
    "# Add edges based on weights (example for fc1)\n",
    "for i in tqdm(range(num_neurons_input)):\n",
    "    for j in range(num_neurons_fc1):\n",
    "        graph.addEdge(i, num_neurons_input + j, fc1_weights[j, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCimqeWrrD6K"
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(num_neurons_fc1)):\n",
    "    for j in range(num_neurons_fc2):\n",
    "        graph.addEdge(num_neurons_input + i, num_neurons_input + num_neurons_fc1 + j, fc2_weights[j, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oejlsgGqUMS"
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(num_neurons_fc2)):\n",
    "    for j in range(num_neurons_fc3):\n",
    "        graph.addEdge(num_neurons_input + num_neurons_fc1 + i,\n",
    "                      num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + j, fc3_weights[j, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgLZo9n5rJFc"
   },
   "outputs": [],
   "source": [
    "# Now you can use the graph object\n",
    "print(\"Nodes\", graph.numberOfNodes())\n",
    "print(\"Edges\", graph.numberOfEdges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEAL1GyXEv7h"
   },
   "source": [
    "# Compute Embedding Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYczeYGa6Qcj"
   },
   "source": [
    "## Estimate Weighted Betweenness for nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8f1jZG76Sbwk"
   },
   "outputs": [],
   "source": [
    "# Compute approximation of betweenness centrality\n",
    "betweenness = nk.centrality.EstimateBetweenness(graph, int(graph.numberOfNodes() * .85), normalized=True, parallel=True)\n",
    "\n",
    "betweenness.run()\n",
    "betweenness_scores = betweenness.scores()\n",
    "\n",
    "# Print or further process the betweenness centrality scores\n",
    "print(\"Estimated Betweenness Centrality Scores:\", betweenness_scores)\n",
    "\n",
    "if not os.path.isdir('./output/'):\n",
    "    os.makedirs('./output')\n",
    "np.save(\"./output/alexnet_estimated_betweenness.npy\", betweenness_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfhuScafIeK7"
   },
   "source": [
    "## Compute weighted degree of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bysgnlcTIlYW"
   },
   "outputs": [],
   "source": [
    "degrees = []\n",
    "\n",
    "for i in tqdm(range(num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + num_neurons_fc3)):\n",
    "    degrees.append(\n",
    "        (abs(graph.weightedDegree(i)) + abs(graph.weightedDegreeIn(i))) / (graph.degreeIn(i) + graph.degreeOut(i)))\n",
    "\n",
    "print(degrees)\n",
    "\n",
    "if not os.path.isdir('./output/'):\n",
    "    os.makedirs('./output')\n",
    "np.save(\"./output/alexnet_weighted_degree.npy\", degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPem8h2wSlWi"
   },
   "outputs": [],
   "source": [
    "norm = np.linalg.norm(degrees)\n",
    "if norm != 0:\n",
    "    normalized_degrees = degrees / norm\n",
    "\n",
    "print(normalized_degrees)\n",
    "\n",
    "if not os.path.isdir('./output/'):\n",
    "    os.makedirs('./output')\n",
    "np.save(\"./output/alexnet_normalized_degree.npy\", normalized_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbJLoG5NNsu-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zm_PEOPWIl0X"
   },
   "source": [
    "## Compute weighted gradient for nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGXjm1eeNPxB"
   },
   "outputs": [],
   "source": [
    "weighted_node_gradients = []\n",
    "\n",
    "for i in tqdm(range(num_neurons_input)):\n",
    "    weighted_node_gradients.append(np.sum(np.abs(np.multiply(fc1_gradients_magnitude[:, i], fc1_weights[:, i]))))\n",
    "\n",
    "for i in tqdm(range(num_neurons_fc1)):\n",
    "    weighted_node_gradients.append(np.sum(np.abs(np.multiply(fc2_gradients_magnitude[:, i], fc2_weights[:, i]))))\n",
    "\n",
    "for i in tqdm(range(num_neurons_fc2)):\n",
    "    weighted_node_gradients.append(np.sum(np.abs(np.multiply(fc3_gradients_magnitude[:, i], fc3_weights[:, i]))))\n",
    "\n",
    "for i in tqdm(range(num_neurons_fc3)):\n",
    "    weighted_node_gradients.append(0.0)\n",
    "\n",
    "print(weighted_node_gradients)\n",
    "\n",
    "if not os.path.isdir('./output/'):\n",
    "    os.makedirs('./output')\n",
    "np.save(\"./output/alexnet_weighted_gradient.npy\", weighted_node_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_Fycnffdr0W"
   },
   "outputs": [],
   "source": [
    "norm = np.linalg.norm(weighted_node_gradients)\n",
    "if norm != 0:\n",
    "    nomralized_weighted_node_gradients = weighted_node_gradients / norm\n",
    "\n",
    "print(nomralized_weighted_node_gradients)\n",
    "\n",
    "if not os.path.isdir('./output/'):\n",
    "    os.makedirs('./output')\n",
    "np.save(\"./output/alexnet_normalized_gradient.npy\", nomralized_weighted_node_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biKm-fpTeUh6"
   },
   "source": [
    "## Add embedding features to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DV0zmvNeXny"
   },
   "outputs": [],
   "source": [
    "node_betweenness = graph.attachNodeAttribute(\"betweenness\", float)\n",
    "node_degree = graph.attachNodeAttribute(\"degree\", float)\n",
    "node_gradient = graph.attachNodeAttribute(\"gradient\", float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp-NRmq82WfQ"
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + num_neurons_fc3)):\n",
    "    node_betweenness[i] = betweenness_scores[i]\n",
    "    node_degree[i] = normalized_degrees[i]\n",
    "    node_gradient[i] = nomralized_weighted_node_gradients[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3eL4MaA2Hq6"
   },
   "source": [
    "## Save complete network to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVG3rbn71pIC"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('./output/'):\n",
    "    os.makedirs('./output')\n",
    "\n",
    "nk.writeGraph(graph, \"./output/alexnet_FC_directed.nkbg003\", nk.Format.NetworkitBinary, chunks=16,\n",
    "              NetworkitBinaryWeights=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPOUMSazEdyL"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Forcibly release memory\n",
    "del train_dataset\n",
    "del test_dataset\n",
    "del train_loader\n",
    "del test_loader\n",
    "del betweenness\n",
    "del graph\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# If using torch, try to clear the GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZotcSen96qHn"
   },
   "source": [
    "# **Building graphs for paths computation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXTYvP4Y6wVo"
   },
   "source": [
    " ## Building network with absolute value of weights for shortest path computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJrvhMRu4Ibv"
   },
   "outputs": [],
   "source": [
    "abs_graph = nk.Graph(num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + num_neurons_fc3, weighted=True,\n",
    "                     directed=True)\n",
    "\n",
    "# FC1\n",
    "for i in tqdm(range(num_neurons_input)):\n",
    "    for j in range(num_neurons_fc1):\n",
    "        abs_graph.addEdge(i, num_neurons_input + j, abs(fc1_weights[j, i]))\n",
    "\n",
    "# FC2\n",
    "for i in tqdm(range(num_neurons_fc1)):\n",
    "    for j in range(num_neurons_fc2):\n",
    "        abs_graph.addEdge(num_neurons_input + i, num_neurons_input + num_neurons_fc1 + j, abs(fc2_weights[j, i]))\n",
    "\n",
    "# FC3\n",
    "for i in tqdm(range(num_neurons_fc2)):\n",
    "    for j in range(num_neurons_fc3):\n",
    "        abs_graph.addEdge(num_neurons_input + num_neurons_fc1 + i,\n",
    "                          num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + j, abs(fc3_weights[j, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4I0AK4v65aT"
   },
   "source": [
    "## Shortest Paths computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "yfpTUmEa68MP"
   },
   "outputs": [],
   "source": [
    "shortest_paths_distances = []  # Store distances separately\n",
    "shortest_paths_nodes = []  # Store paths (nodes) separately\n",
    "\n",
    "heuristic = [0 for _ in range(abs_graph.upperNodeIdBound())]\n",
    "\n",
    "for i in tqdm(range(0, num_neurons_input)):\n",
    "    # print(\"searching for node\",i)\n",
    "    min_distance = float('inf')\n",
    "    min_path = []\n",
    "    dijkstra = nk.distance.Dijkstra(abs_graph, source=i, storePaths=True)\n",
    "    dijkstra.run()\n",
    "\n",
    "    for j in range(num_neurons_input + num_neurons_fc1 + num_neurons_fc2,\n",
    "                   num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + num_neurons_fc3):\n",
    "        # print(\"To node\",j)\n",
    "        if (dijkstra.distance(j) < min_distance):\n",
    "            min_distance = dijkstra.distance(j)\n",
    "            min_path = dijkstra.getPath(j)\n",
    "\n",
    "    # print(\"[\",min_distance,\",\",min_path,\"]\")\n",
    "    shortest_paths_distances.append(min_distance)\n",
    "    shortest_paths_nodes.append(min_path)\n",
    "\n",
    "if not os.path.isdir('./output/'):\n",
    "    os.makedirs('./output')\n",
    "np.save(\"./output/alexnet_shortest_paths_distances.npy\", shortest_paths_distances)\n",
    "np.save(\"./output/alexnet_shortest_paths_nodes.npy\", shortest_paths_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjBiUBZX_dq7"
   },
   "outputs": [],
   "source": [
    "del (abs_graph)\n",
    "del (shortest_paths_distances)\n",
    "del (shortest_paths_nodes)\n",
    "del (dijkstra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnCcXtEk8DUo"
   },
   "source": [
    "## Inverting weights to find longest paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MEsBVBm8Cxl"
   },
   "outputs": [],
   "source": [
    "abs_graph = nk.Graph(num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + num_neurons_fc3, weighted=True,\n",
    "                     directed=True)\n",
    "\n",
    "# FC1\n",
    "for i in tqdm(range(num_neurons_input)):\n",
    "    for j in range(num_neurons_fc1):\n",
    "        abs_graph.addEdge(i, num_neurons_input + j, -abs(fc1_weights[j, i]))\n",
    "\n",
    "# FC2\n",
    "for i in tqdm(range(num_neurons_fc1)):\n",
    "    for j in range(num_neurons_fc2):\n",
    "        abs_graph.addEdge(num_neurons_input + i, num_neurons_input + num_neurons_fc1 + j, -abs(fc2_weights[j, i]))\n",
    "\n",
    "# FC3\n",
    "for i in tqdm(range(num_neurons_fc2)):\n",
    "    for j in range(num_neurons_fc3):\n",
    "        abs_graph.addEdge(num_neurons_input + num_neurons_fc1 + i,\n",
    "                          num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + j, -abs(fc3_weights[j, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7WFNUBj8NjD"
   },
   "source": [
    "## Finding longest_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ZyfYEMzi8P6j"
   },
   "outputs": [],
   "source": [
    "longest_paths_distances = []  # Store distances separately\n",
    "longest_paths_nodes = []  # Store paths (nodes) separately\n",
    "\n",
    "heuristic = [0 for _ in range(abs_graph.upperNodeIdBound())]\n",
    "\n",
    "for i in tqdm(range(0, num_neurons_input)):\n",
    "    # print(\"searching for node\",i)\n",
    "    min_distance = float('-inf')\n",
    "    min_path = []\n",
    "    dijkstra = nk.distance.Dijkstra(abs_graph, source=i, storePaths=True)\n",
    "    dijkstra.run()\n",
    "\n",
    "    for j in range(num_neurons_input + num_neurons_fc1 + num_neurons_fc2,\n",
    "                   num_neurons_input + num_neurons_fc1 + num_neurons_fc2 + num_neurons_fc3):\n",
    "        # print(\"To node\",j)\n",
    "        if (dijkstra.distance(j) > min_distance):\n",
    "            min_distance = dijkstra.distance(j)\n",
    "            min_path = dijkstra.getPath(j)\n",
    "\n",
    "    # print(\"[\",-min_distance,\",\",min_path,\"]\")\n",
    "    longest_paths_distances.append(-min_distance)\n",
    "    longest_paths_nodes.append(min_path)\n",
    "\n",
    "if not os.path.isdir('./output/'):\n",
    "    os.makedirs('./output')\n",
    "np.save(\"./output/alexnet_longest_paths_distances.npy\", longest_paths_distances)\n",
    "np.save(\"./output/alexnet_longest_paths_nodes.npy\", longest_paths_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPDtyuwp_lpB"
   },
   "outputs": [],
   "source": [
    "del (abs_graph)\n",
    "del (longest_paths_distances)\n",
    "del (longest_paths_nodes)\n",
    "del (dijkstra)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compress Output Folder"
   ],
   "metadata": {
    "id": "z_m6Zo1qcwPV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!zip -r /content/AlexNet_Results.zip /content/output"
   ],
   "metadata": {
    "id": "UP82Zb_-ct1N"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
